{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Layer\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from scipy.stats import percentileofscore\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../pipeline/tmp/cities/poz-w-dataset.parquet')\n",
    "#df = pd.read_parquet('poz-w-dataset.parquet')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['to_x']\n",
    "y = df['to_y']\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.hist2d(x, y, bins=100, cmap='viridis')\n",
    "\n",
    "plt.colorbar(label='Counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['from_x', 'from_y', 'to_x', 'to_y', 'day_type', 'start']]\n",
    "y = df['time']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomScalerLayer(Layer):\n",
    "    def __init__(self, city_diameter=25000, **kwargs):\n",
    "        super(CustomScalerLayer, self).__init__(**kwargs)\n",
    "        self.city_diameter = city_diameter\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Normalize coordinates\n",
    "        from_x = (inputs[:, 0] / self.city_diameter) + 0.5\n",
    "        from_y = (inputs[:, 1] / self.city_diameter) + 0.5\n",
    "        to_x = (inputs[:, 2] / self.city_diameter) + 0.5\n",
    "        to_y = (inputs[:, 3] / self.city_diameter) + 0.5\n",
    "\n",
    "        # Normalize start time\n",
    "        start = inputs[:, 5] / 86400.0  # 86400 seconds in a day\n",
    "\n",
    "        # Combine normalized features\n",
    "        normalized_inputs = tf.stack([from_x, from_y, to_x, to_y, inputs[:, 4], start], axis=1)\n",
    "        return normalized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_layer = CustomScalerLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    scaler_layer,\n",
    "    Dense(64, activation='relu', kernel_initializer=HeNormal()),\n",
    "    #Dense(64, activation='relu', kernel_initializer=HeNormal(),input_shape=(X_train.shape[1],)),\n",
    "    #Dense(128, activation='relu', kernel_initializer=HeNormal()),\n",
    "    #Dense(256, activation='relu', kernel_initializer=HeNormal()),\n",
    "    #Dense(128, activation='relu', kernel_initializer=HeNormal()),\n",
    "    Dense(64, activation='relu', kernel_initializer=HeNormal()),\n",
    "    Dense(16, activation='relu', kernel_initializer=HeNormal()),\n",
    "    Dense(4, activation='relu', kernel_initializer=HeNormal()),\n",
    "    Dense(1, activation='exponential')\n",
    "    #Dense(1, activation='softplus')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 2:\n",
    "        return lr * 0.1\n",
    "    else:\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLearningRateScheduler(Callback):\n",
    "    def __init__(self, total_batches):\n",
    "        super(BatchLearningRateScheduler, self).__init__()\n",
    "        self.total_batches = total_batches\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.current_epoch = epoch\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        if self.current_epoch == 0 and batch < self.total_batches:\n",
    "            lr = tf.keras.backend.get_value(self.model.optimizer.lr)\n",
    "            ratio = (batch + 1) / self.total_batches\n",
    "            divisor = ratio if ratio > 0.1 else 0.1\n",
    "            new_lr = lr * divisor\n",
    "            tf.keras.backend.set_value(self.model.optimizer.lr, new_lr)\n",
    "        elif self.current_epoch > 0:\n",
    "            lr = tf.keras.backend.get_value(self.model.optimizer.lr)\n",
    "            tf.keras.backend.set_value(self.model.optimizer.lr, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batches = 3000\n",
    "batch_lr_scheduler = BatchLearningRateScheduler(total_batches)\n",
    "simple_lr_scheduler = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asymetric_loss_function(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "    tf.debugging.assert_greater(\n",
    "        tf.reduce_min(y_true), tf.constant(0.0, dtype=tf.float32),\n",
    "        message=\"y_true contains zero or negative values!\"\n",
    "    )\n",
    "\n",
    "    ratio = y_pred / (y_true * 0.9)\n",
    "\n",
    "    tf.debugging.assert_non_negative(\n",
    "        ratio,\n",
    "        message=\"Ratio is negative!\"\n",
    "    )\n",
    "    loss = tf.where(ratio <= 1, (1-ratio)**2, 7* tf.math.log(ratio))\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=asymetric_loss_function, metrics=['mape'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=8, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,  # Enable TensorFlow Lite ops.\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS  # Enable TensorFlow Select ops.\n",
    "]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read tflite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.tflite', 'rb') as f:\n",
    "    tflite_model = f.read()\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "interpreter.resize_tensor_input(input_details[0]['index'], (1, 6))\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predicitons on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    # single_row = X_test.iloc[i].to_numpy().reshape(1, -1).astype(np.float64)\n",
    "    single_row = X_test.iloc[i].to_numpy().reshape(1, -1).astype(np.float32)\n",
    "    interpreter.set_tensor(input_details[0]['index'], single_row)\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    y_pred.append(output_data[0][0])\n",
    "\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(f'Mean Absolute Percentage Error (MAPE): {mape:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize set results distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_difference = ((y_pred - y_test) / y_test) * 100\n",
    "\n",
    "percentile_1 = np.percentile(percentage_difference, 1)\n",
    "percentile_99 = np.percentile(percentage_difference, 99.9)\n",
    "max_overestimation = np.max(percentage_difference)\n",
    "percentile_of_0 = percentileofscore(percentage_difference, 0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(percentage_difference, kde=True, bins=100, color='blue')\n",
    "\n",
    "plt.axvline(percentile_1, color='green', linestyle='--', linewidth=2, label=f'1st Percentile: {percentile_1:.2f}%')\n",
    "plt.axvline(percentile_99, color='red', linestyle='--', linewidth=2, label=f'99.9th Percentile: {percentile_99:.2f}%')\n",
    "\n",
    "plt.text(percentile_1, plt.ylim()[1] * 0.9, f'{percentile_1:.2f}%', color='green', ha='center')\n",
    "plt.text(percentile_99, plt.ylim()[1] * 0.9, f'{percentile_99:.2f}%', color='red', ha='center')\n",
    "\n",
    "plt.axvline(max_overestimation, color='purple', linestyle='--', linewidth=2, label=f'Max Overestimation: {max_overestimation:.2f}%')\n",
    "plt.text(max_overestimation, plt.ylim()[1] * 0.8, f'{max_overestimation:.2f}%', color='purple', ha='center')\n",
    "\n",
    "plt.xlabel('Percentage Difference (%)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Percentage Difference Between Predictions and Targets')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f'Maximal Overestimation Value: {max_overestimation:.2f}%')\n",
    "print(f'Percentile for 0 on x-axis value: {percentile_of_0:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 10000\n",
    "total_time = 0\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    random_index = np.random.randint(0, X_test.shape[0])\n",
    "    # single_row = X_test.iloc[i].to_numpy().reshape(1, -1).astype(np.float64)\n",
    "    single_row = X_test.iloc[i].to_numpy().reshape(1, -1).astype(np.float32)\n",
    "    start_time = time.perf_counter()\n",
    "    interpreter.set_tensor(input_details[0]['index'], single_row)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    end_time = time.perf_counter()\n",
    "    total_time += (end_time - start_time)\n",
    "\n",
    "average_forward_pass_time = (total_time / num_iterations) * 1e6\n",
    "print(f\"Average forward pass time for a single row using TensorFlow Lite: {average_forward_pass_time:.6f} microseconds\")\n",
    "print(f\"total time for 10000 rows: {total_time*1000} milliseconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
